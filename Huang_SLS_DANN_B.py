"""
File â€” Huang + SLS + DANNï¼ˆç„¡ fine-tuneï¼ŒScenario Bï¼‰
=====================================================
æ¶æ§‹èªªæ˜ï¼š
  - Wav2Vec2 ä¸»å¹¹ï¼šå®Œå…¨å‡çµï¼ˆåŒ…å« CNN feature extractor + Transformer encoderï¼‰
  - SLS (Stochastic Layer Selection)ï¼šå¯å­¸ç¿’çš„åŠ æ¬Šèåˆæ‰€æœ‰ hidden states
  - DANN domain classifierï¼šå°æŠ—å¼è¨“ç·´ï¼Œä»¥ speaker ç‚º domain
  - dep_classifierï¼šäºŒå…ƒæ†‚é¬±åˆ†é¡ (binary)

ä¿®æ­£æ¸…å–®ï¼ˆç›¸å°æ–¼è‰ç¨¿ v4ï¼‰ï¼š
  1. SEED=42 â†’ 103ï¼Œå°é½Šè«–æ–‡åŸºæº–
  2. EVAL/SAVE/LOGGING_STEPS=50 â†’ 10ï¼Œå°é½Šè«–æ–‡åŸºæº–
  3. run seedï¼šSEED+run_i â†’ SEED+run_i-1ï¼ˆ103,104,105,106,107ï¼‰
  4. pth æª”åï¼šhuang_sls_dann_Aï¼ˆåŸæœ¬å·²æ­£ç¢ºï¼‰
  5. å½™ç¸½ï¼šæ–°å¢ summary_5runs.csv + results["run"]=run_i + meanÂ±std æ ¼å¼
  6. dataloader_drop_last=True ç§»é™¤ï¼ˆeval æ™‚æ¨£æœ¬æ•¸å¯èƒ½ä¸æ•´é™¤ batchï¼Œdrop æœƒä¸Ÿè³‡æ–™ï¼‰
  7. spk_classifier dimï¼šhardcode 200 â†’ num_speakersï¼ˆå¾è³‡æ–™å‹•æ…‹è¨ˆç®—ï¼‰
  8. gc.collect é‡‹æ”¾è¨˜æ†¶é«”
"""

import os
import gc
import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torchaudio
import matplotlib.pyplot as plt

from dataclasses import dataclass
from typing import Optional, Tuple, Dict, List, Union, Any
from math import sqrt, exp

from torch.autograd import Function
from datasets import Dataset as HFDataset
from transformers import (
    Wav2Vec2Processor,
    Wav2Vec2Config,
    Wav2Vec2PreTrainedModel,
    Wav2Vec2Model,
    Trainer,
    TrainingArguments,
    EvalPrediction,
    set_seed,
)
from transformers.file_utils import ModelOutput
from packaging import version
from sklearn.metrics import (
    accuracy_score, f1_score, classification_report,
    confusion_matrix, roc_curve, auc, mean_squared_error,
)

# ============================================================
#  è¨­å®šå€
# ============================================================
TRAIN_CSV  = "./experiment_sisman_scientific/scenario_B_monitoring/train.csv"
TEST_CSV   = "./experiment_sisman_scientific/scenario_B_monitoring/test.csv"
AUDIO_ROOT = "/export/fs05/hyeh10/depression/daic_5utt_full/merged_5"

MODEL_NAME = "facebook/wav2vec2-base"
OUTPUT_DIR = "./output_huang_sls_dann_B"

SEED             = 103   # ä¿®æ­£ 1ï¼šå°é½Šè«–æ–‡åŸºæº–
TOTAL_RUNS       = 5
NUM_EPOCHS       = 10
LEARNING_RATE    = 1e-4
BATCH_SIZE       = 4
GRAD_ACCUM       = 2
EVAL_STEPS       = 10    # ä¿®æ­£ 2ï¼šå°é½Šè«–æ–‡åŸºæº–
SAVE_STEPS       = 10
LOGGING_STEPS    = 10
SAVE_TOTAL_LIMIT = 2
FP16 = torch.cuda.is_available()

LABEL_MAP   = {"non": 0, "0": 0, 0: 0, "dep": 1, "1": 1, 1: 1}
LABEL_NAMES = ["non-depressed", "depressed"]
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"


# ============================================================
#  æ¨¡å‹å®šç¾©
# ============================================================

@dataclass
class SpeechClassifierOutput(ModelOutput):
    loss:           Optional[torch.FloatTensor]        = None
    logits:         torch.FloatTensor                  = None
    speaker_logits: Optional[torch.FloatTensor]        = None
    hidden_states:  Optional[Tuple[torch.FloatTensor]] = None
    attentions:     Optional[Tuple[torch.FloatTensor]] = None


class GradientReversalFn(Function):
    @staticmethod
    def forward(ctx, x, alpha):
        ctx.alpha = alpha
        return x.view_as(x)

    @staticmethod
    def backward(ctx, grad_output):
        return grad_output.neg() * ctx.alpha, None


class Wav2Vec2_SLS_DANN(Wav2Vec2PreTrainedModel):
    """
    Wav2Vec2 + SLS + DANNï¼Œç„¡ fine-tune
    - wav2vec2 å…¨éƒ¨å‡çµ
    - SLS åŠ æ¬Šèåˆæ‰€æœ‰ hidden statesï¼ˆ13 å±¤ï¼‰
    - dep_classifier + spk_classifierï¼ˆGRL å°æŠ—ï¼‰
    - alpha ç”± CTCTrainer å‹•æ…‹æ³¨å…¥ï¼ˆself._alphaï¼‰
    """
    def __init__(self, config):
        super().__init__(config)
        self.wav2vec2 = Wav2Vec2Model(config)

        for param in self.wav2vec2.parameters():
            param.requires_grad = False

        num_layers = config.num_hidden_layers + 1
        self.sls_weights = nn.Parameter(torch.ones(num_layers))

        self.down_proj = nn.Sequential(
            nn.Linear(config.hidden_size, 128),
            nn.BatchNorm1d(128),
            nn.ReLU(),
            nn.Dropout(0.3),
        )
        self.dep_classifier = nn.Linear(128, config.num_labels)
        # ä¿®æ­£ 7ï¼šç”¨ config å‹•æ…‹å– num_speakersï¼Œä¸ hardcode 200
        self.spk_classifier = nn.Linear(128, getattr(config, "num_speakers", 151))

        self._alpha = 0.0
        self.init_weights()

    def freeze_feature_extractor(self):
        pass

    def forward(
        self,
        input_values,
        attention_mask=None,
        labels=None,
        speaker_labels=None,
        output_attentions=None,
        output_hidden_states=None,
        return_dict=None,
    ):
        outputs = self.wav2vec2(
            input_values,
            attention_mask=attention_mask,
            output_hidden_states=True,
            output_attentions=output_attentions,
            return_dict=True,
        )

        hidden_states = torch.stack(outputs.hidden_states)        # [L, B, T, H]
        weights       = torch.softmax(self.sls_weights, dim=0)    # [L]
        fused         = (hidden_states * weights.view(-1, 1, 1, 1)).sum(0)  # [B, T, H]

        shared     = self.down_proj(torch.mean(fused, dim=1))     # [B, 128]
        dep_logits = self.dep_classifier(shared)
        spk_logits = self.spk_classifier(
            GradientReversalFn.apply(shared, self._alpha)
        )

        loss = None
        if labels is not None:
            loss = nn.CrossEntropyLoss()(dep_logits, labels)
        if speaker_labels is not None:
            mask = speaker_labels >= 0
            if mask.sum() > 0:
                spk_loss = nn.CrossEntropyLoss()(
                    spk_logits[mask].view(-1, spk_logits.size(-1)),
                    speaker_labels[mask].view(-1)
                )
                loss = loss + self._alpha * spk_loss if loss is not None else spk_loss

        return SpeechClassifierOutput(
            loss=loss, logits=dep_logits, speaker_logits=spk_logits,
            hidden_states=None, attentions=None,
        )


# ============================================================
#  DataCollatorï¼ˆå« speaker_labelsï¼‰
# ============================================================

@dataclass
class DataCollatorWithSpeaker:
    processor: Wav2Vec2Processor
    padding: Union[bool, str] = True

    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:
        batch = self.processor.pad(
            [{"input_values": f["input_values"]} for f in features],
            padding=self.padding, return_tensors="pt",
        )
        batch["labels"]         = torch.tensor([f["labels"]         for f in features], dtype=torch.long)
        batch["speaker_labels"] = torch.tensor([f["speaker_labels"] for f in features], dtype=torch.long)
        return batch


# ============================================================
#  compute_metrics
# ============================================================

def compute_metrics(p: EvalPrediction):
    preds = p.predictions[0] if isinstance(p.predictions, tuple) else p.predictions
    preds = np.argmax(preds, axis=1)
    return {
        "accuracy": accuracy_score(p.label_ids, preds),
        "f1":       f1_score(p.label_ids, preds, average="macro"),
    }


# ============================================================
#  CTCTrainer â€” å‹•æ…‹æ³¨å…¥ alpha
# ============================================================

if version.parse(torch.__version__) >= version.parse("1.6"):
    from torch.cuda.amp import autocast


class CTCTrainer(Trainer):
    def training_step(self, model: nn.Module, inputs: Dict[str, Union[torch.Tensor, Any]]) -> torch.Tensor:
        total_steps = self.args.max_steps if self.args.max_steps > 0 else (
            len(self.train_dataset) //
            (self.args.per_device_train_batch_size * self.args.gradient_accumulation_steps)
            * int(self.args.num_train_epochs)
        )
        p     = float(self.state.global_step) / max(total_steps, 1)
        alpha = 2.0 / (1.0 + exp(-10.0 * p)) - 1.0
        model._alpha = alpha

        model.train()
        inputs = self._prepare_inputs(inputs)

        is_amp_used = self.args.fp16 or self.args.bf16
        if is_amp_used:
            with torch.amp.autocast("cuda"):
                loss = self.compute_loss(model, inputs)
        else:
            loss = self.compute_loss(model, inputs)

        if self.args.gradient_accumulation_steps > 1:
            loss = loss / self.args.gradient_accumulation_steps

        if is_amp_used:
            if hasattr(self, "scaler") and self.scaler is not None:
                self.scaler.scale(loss).backward()
            elif hasattr(self, "accelerator"):
                self.accelerator.backward(loss)
            else:
                loss.backward()
        else:
            loss.backward()

        return loss.detach()


# ============================================================
#  è³‡æ–™è¼‰å…¥èˆ‡é è™•ç†
# ============================================================

def extract_speaker_id(filepath: str) -> str:
    return os.path.basename(filepath).split("_")[0]


def load_audio_dataset(csv_path: str, speaker_to_idx: dict = None, is_train: bool = True):
    df = pd.read_csv(csv_path)
    print(f"ğŸ“‚ è®€å– {csv_path}ï¼Œå…± {len(df)} ç­†è³‡æ–™")

    if is_train and speaker_to_idx is None:
        # Scenario Bï¼šspeaker map åªå« 38 ä½ target speakersï¼ˆå¾ TEST_CSV å»ºç«‹ï¼‰
        # 151 ä½ base speakers â†’ spk_idx=-1ï¼Œä¸åƒèˆ‡ L_spk
        import pandas as _pd_tmp
        test_df = _pd_tmp.read_csv(TEST_CSV)
        target_speakers = sorted(set(extract_speaker_id(p) for p in test_df["path"].tolist()))
        speaker_to_idx  = {spk: idx for idx, spk in enumerate(target_speakers)}
        print(f"ğŸ” åµæ¸¬åˆ° {len(speaker_to_idx)} ä½ target speakerï¼ˆå¾ TEST_CSVï¼Œæ‡‰ç‚º 38ï¼‰")

    records, skipped = [], 0
    for _, row in df.iterrows():
        wav_path  = os.path.join(AUDIO_ROOT, row["path"])
        raw_label = str(row["label"]).strip().lower()
        if raw_label not in LABEL_MAP:
            skipped += 1; continue
        if not os.path.exists(wav_path):
            print(f"âš ï¸ ä¸å­˜åœ¨: {wav_path}"); skipped += 1; continue
        spk_idx = speaker_to_idx.get(extract_speaker_id(wav_path), -1)
        records.append({"path": wav_path, "label": LABEL_MAP[raw_label], "speaker_labels": spk_idx})

    if skipped: print(f"âš ï¸ è·³é {skipped} ç­†")
    print(f"âœ… è¼‰å…¥ {len(records)} ç­†")
    return HFDataset.from_dict({
        "path":           [r["path"]           for r in records],
        "label":          [r["label"]          for r in records],
        "speaker_labels": [r["speaker_labels"] for r in records],
    }), speaker_to_idx


def speech_file_to_array_fn(batch, processor):
    speech, sr = torchaudio.load(batch["path"])
    if speech.shape[0] > 1:
        speech = torch.mean(speech, dim=0, keepdim=True)
    speech = speech.squeeze().numpy()
    if sr != 16000:
        import librosa
        speech = librosa.resample(speech, orig_sr=sr, target_sr=16000)
    batch["speech"] = speech
    return batch


def preprocess_function(batch, processor):
    result = processor(batch["speech"], sampling_rate=16000, return_tensors="np", padding=False)
    batch["input_values"] = result.input_values[0]
    batch["labels"]       = batch["label"]
    return batch


# ============================================================
#  è©•ä¼°
# ============================================================

def full_evaluation(trainer, test_dataset, output_dir, run_i):
    predictions = trainer.predict(test_dataset)
    preds  = predictions.predictions
    if isinstance(preds, tuple): preds = preds[0]
    y_pred = np.argmax(preds, axis=1)
    y_true = predictions.label_ids

    results_path = os.path.join(output_dir, f"results_run{run_i}")
    os.makedirs(results_path, exist_ok=True)

    report    = classification_report(y_true, y_pred, target_names=LABEL_NAMES, zero_division=0, output_dict=True)
    report_df = pd.DataFrame(report).transpose()
    cm_df     = pd.DataFrame(confusion_matrix(y_true, y_pred), index=LABEL_NAMES, columns=LABEL_NAMES)
    mse = mean_squared_error(y_true, y_pred)
    report_df["MSE"]  = mse
    report_df["RMSE"] = sqrt(mse)
    report_df.to_csv(os.path.join(results_path, "clsf_report.csv"), sep="\t")
    cm_df.to_csv(    os.path.join(results_path, "conf_matrix.csv"), sep="\t")

    fpr, tpr, _ = roc_curve(y_true, y_pred)
    roc_auc     = auc(fpr, tpr)
    plt.figure()
    plt.plot(fpr, tpr, color="darkorange", lw=2, label=f"ROC (AUC={roc_auc:.2f})")
    plt.plot([0, 1], [0, 1], color="navy", lw=2, linestyle="--")
    plt.xlim([0, 1]); plt.ylim([0, 1.05])
    plt.xlabel("FPR"); plt.ylabel("TPR")
    plt.title(f"ROC Curve - Scenario B Run {run_i}")
    plt.legend(); plt.savefig(os.path.join(results_path, "roc_curve.png")); plt.close()

    acc = accuracy_score(y_true, y_pred)
    f1  = f1_score(y_true, y_pred, average="macro")
    print(f"\nğŸ¯ Run {run_i}: Acc={acc:.4f} | F1={f1:.4f} | AUC={roc_auc:.4f}")
    return {"accuracy": acc, "f1": f1, "auc": roc_auc}


# ============================================================
#  ä¸»ç¨‹å¼
# ============================================================

if __name__ == "__main__":
    print("=" * 60)
    print("ğŸš€ Huang + SLS + DANNï¼ˆç„¡ fine-tuneï¼‰â€” Scenario B")
    print(f"   SEED={SEED} | LR={LEARNING_RATE} | Epochs={NUM_EPOCHS} | Runs={TOTAL_RUNS}")
    print("=" * 60)

    set_seed(SEED)
    processor = Wav2Vec2Processor.from_pretrained(MODEL_NAME)

    print("\nğŸ“¦ è¼‰å…¥è³‡æ–™é›†ï¼ˆåªåŸ·è¡Œä¸€æ¬¡ï¼‰...")
    train_dataset_full, speaker_to_idx = load_audio_dataset(TRAIN_CSV, is_train=True)
    test_dataset_raw, _ = load_audio_dataset(TEST_CSV, speaker_to_idx=speaker_to_idx, is_train=False)
    num_speakers = len(speaker_to_idx)
    print(f"ğŸ‘¥ å…± {num_speakers} ä½ speaker")

    print("\nğŸ”Š é è™•ç†éŸ³è¨Šï¼ˆåªåŸ·è¡Œä¸€æ¬¡ï¼‰...")
    map_kwargs = {"fn_kwargs": {"processor": processor}}
    train_dataset_full = train_dataset_full.map(speech_file_to_array_fn, **map_kwargs)
    test_dataset_raw   = test_dataset_raw.map(  speech_file_to_array_fn, **map_kwargs)
    train_dataset_full = train_dataset_full.map(preprocess_function,      **map_kwargs)
    test_dataset       = test_dataset_raw.map(  preprocess_function,      **map_kwargs)

    all_results = []
    for run_i in range(1, TOTAL_RUNS + 1):
        print(f"\n{'='*60}")
        print(f"ğŸ¬ Run {run_i} / {TOTAL_RUNS}")
        print(f"{'='*60}")

        # ä¿®æ­£ 3ï¼šseed 103, 104, 105, 106, 107
        run_seed = SEED + run_i - 1
        set_seed(run_seed)
        print(f"ğŸ² Run {run_i} seed: {run_seed}")
        print(f"ğŸ“Š Train: {len(train_dataset_full)} | Test(eval): {len(test_dataset)}")

        config = Wav2Vec2Config.from_pretrained(
            MODEL_NAME,
            num_labels=2,
            num_speakers=num_speakers,   # ä¿®æ­£ 7ï¼šå‹•æ…‹è¨­å®š
            final_dropout=0.1,
            output_hidden_states=True,
        )
        model = Wav2Vec2_SLS_DANN.from_pretrained(MODEL_NAME, config=config)

        frozen    = sum(1 for p in model.wav2vec2.parameters() if not p.requires_grad)
        total     = sum(1 for p in model.wav2vec2.parameters())
        trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)
        print(f"â„ï¸  wav2vec2 å‡çµ: {frozen}/{total} | ğŸ”¥ å¯è¨“ç·´: {trainable:,}")

        run_output_dir = os.path.join(OUTPUT_DIR, f"run_{run_i}")
        os.makedirs(run_output_dir, exist_ok=True)

        training_args = TrainingArguments(
            output_dir=run_output_dir,
            per_device_train_batch_size=BATCH_SIZE,
            per_device_eval_batch_size=BATCH_SIZE,
            gradient_accumulation_steps=GRAD_ACCUM,
            evaluation_strategy="steps",
            num_train_epochs=NUM_EPOCHS,
            fp16=FP16,
            save_steps=SAVE_STEPS,
            eval_steps=EVAL_STEPS,
            logging_steps=LOGGING_STEPS,
            learning_rate=LEARNING_RATE,
            save_total_limit=SAVE_TOTAL_LIMIT,
            seed=run_seed,
            data_seed=run_seed,
            load_best_model_at_end=True,
            # metric_for_best_model æœªè¨­å®š â†’ é è¨­ eval_lossï¼Œå°é½Šè«–æ–‡
            report_to="none",
            remove_unused_columns=False,
            # ä¿®æ­£ 6ï¼šç§»é™¤ dataloader_drop_lastï¼ˆeval æ™‚ drop æœƒä¸Ÿæ‰å°¾éƒ¨æ¨£æœ¬ï¼‰
        )

        trainer = CTCTrainer(
            model=model,
            data_collator=DataCollatorWithSpeaker(processor=processor, padding=True),
            args=training_args,
            compute_metrics=compute_metrics,
            train_dataset=train_dataset_full,
            eval_dataset=test_dataset,
            tokenizer=processor.feature_extractor,
        )

        print("âš”ï¸ é–‹å§‹è¨“ç·´...")
        try:
            trainer.train()
        except RuntimeError as e:
            if "out of memory" in str(e):
                print("âš ï¸ OOMï¼Œè·³éæ­¤ run")
                torch.cuda.empty_cache(); continue
            raise e

        best_path = os.path.join(run_output_dir, "best_model")
        trainer.save_model(best_path)
        processor.save_pretrained(best_path)
        print(f"ğŸ’¾ æœ€ä½³æ¨¡å‹å„²å­˜è‡³: {best_path}")

        # ä¿®æ­£ 4ï¼špth æª”åå« Aï¼ˆScenario Bï¼‰
        pth_path = os.path.join(OUTPUT_DIR, f"huang_sls_dann_B_shared_encoder_run_{run_i}.pth")
        torch.save(trainer.model.down_proj.state_dict(), pth_path)
        print(f"ğŸ”‘ down_proj .pth å„²å­˜è‡³: {pth_path}")

        results = full_evaluation(trainer, test_dataset, OUTPUT_DIR, run_i)
        results["run"] = run_i   # ä¿®æ­£ 5a
        all_results.append(results)
        print(f"Run {run_i} â†’ Acc: {results['accuracy']:.4f} | F1: {results['f1']:.4f} | AUC: {results['auc']:.4f}")

        del model, trainer; torch.cuda.empty_cache(); gc.collect()  # ä¿®æ­£ 8

    # â”€â”€ è·¨ run çµ±è¨ˆ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    print(f"\n{'='*60}")
    print(f"ğŸ“ˆ Scenario B SLS+DANN â€” {TOTAL_RUNS} æ¬¡å¯¦é©—å½™ç¸½")
    print(f"{'='*60}")
    if all_results:
        results_df = pd.DataFrame(all_results)
        print(results_df.to_string(index=False))
        for metric in ["accuracy", "f1", "auc"]:
            vals = results_df[metric].values
            print(f"  {metric.upper():10s}  mean={np.mean(vals):.4f} Â± {np.std(vals):.4f}  "
                  f"min={np.min(vals):.4f}  max={np.max(vals):.4f}")
        summary_path = os.path.join(OUTPUT_DIR, "summary_5runs.csv")
        results_df.to_csv(summary_path, index=False)   # ä¿®æ­£ 5b
        print(f"\nâœ… å½™ç¸½å·²å„²å­˜è‡³ {summary_path}")

    print("\nğŸ Scenario B å¯¦é©—å®Œæˆï¼")