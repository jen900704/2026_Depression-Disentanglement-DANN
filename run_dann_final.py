import os
import pandas as pd
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
from torch.autograd import Function
from transformers import Wav2Vec2Processor, Wav2Vec2Model
import torchaudio
from tqdm import tqdm
from sklearn.metrics import accuracy_score, f1_score
from sklearn.manifold import TSNE
import matplotlib.pyplot as plt
import seaborn as sns

# ==========================================
# ğŸ”§ 1. è¨­å®šå€ (Config)
# ==========================================
TRAIN_CSV_PATH = "./experiment_sisman_scientific/scenario_B_monitoring/train.csv"
TEST_CSV_PATH = "./experiment_sisman_scientific/scenario_B_monitoring/test.csv"
AUDIO_ROOT = "" 

MODEL_NAME = "facebook/wav2vec2-base"
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
BATCH_SIZE = 32
EPOCHS = 30
RUN_NAME = "Final_Defense" # ç”¨æ–¼æª”å

print(f"ğŸ–¥ï¸ ä½¿ç”¨è£ç½®: {DEVICE}")

# ==========================================
# ğŸ§  2. æ¨¡å‹å®šç¾© (DANN Architecture)
# ==========================================
class GradientReversalFn(Function):
    @staticmethod
    def forward(ctx, x, alpha):
        ctx.alpha = alpha
        return x.view_as(x)
    @staticmethod
    def backward(ctx, grad_output):
        output = grad_output.neg() * ctx.alpha
        return output, None

class GradientReversalLayer(nn.Module):
    def __init__(self):
        super(GradientReversalLayer, self).__init__()
    def forward(self, x, alpha=1.0):
        return GradientReversalFn.apply(x, alpha)

class DANN_Model(nn.Module):
    def __init__(self, input_dim=768, hidden_dim=128, num_classes=2, num_speakers=38):
        super(DANN_Model, self).__init__()
        self.shared_encoder = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.BatchNorm1d(hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.3)
        )
        self.class_classifier = nn.Sequential(
            nn.Linear(hidden_dim, 64),
            nn.ReLU(),
            nn.Linear(64, num_classes)
        )
        self.domain_classifier = nn.Sequential(
            nn.Linear(hidden_dim, 64),
            nn.ReLU(),
            nn.Linear(64, num_speakers)
        )
        self.grl = GradientReversalLayer()

    def forward(self, x, alpha=1.0):
        features = self.shared_encoder(x)
        class_output = self.class_classifier(features)
        reverse_features = self.grl(features, alpha)
        domain_output = self.domain_classifier(reverse_features)
        return class_output, domain_output

# ==========================================
# ğŸ“‚ 3. è³‡æ–™è™•ç†å·¥å…·
# ==========================================
def extract_speaker_id(filepath):
    filename = os.path.basename(filepath)
    speaker_id = filename.split('_')[0] 
    return speaker_id

def prepare_data(csv_path, processor, model, speaker_to_idx=None, is_train=True):
    df = pd.read_csv(csv_path)
    print(f"ğŸ“‚ æ­£åœ¨è™•ç† {csv_path} (å…± {len(df)} ç­†)...")
    
    features_list = []
    labels_list = []
    speaker_indices_list = []
    
    label_map = {'dep': 1, '1': 1, 1: 1, 'non': 0, '0': 0, 0: 0}

    if is_train and speaker_to_idx is None:
        all_speakers = df['path'].apply(extract_speaker_id).unique()
        all_speakers = sorted(all_speakers)
        speaker_to_idx = {spk: idx for idx, spk in enumerate(all_speakers)}
        print(f"ğŸ” [è¨“ç·´é›†] Speaker Map: {list(speaker_to_idx.items())[:5]}...")
    
    model.eval()
    with torch.no_grad():
        for _, row in tqdm(df.iterrows(), total=len(df), desc="Extracting Features"):
            wav_path = os.path.join(AUDIO_ROOT, row['path'])
            try:
                waveform, sample_rate = torchaudio.load(wav_path)
                if sample_rate != 16000:
                    waveform = torchaudio.transforms.Resample(sample_rate, 16000)(waveform)
                if waveform.shape[0] > 1: waveform = torch.mean(waveform, dim=0, keepdim=True)
                
                raw_label = str(row['label']).strip().lower()
                if raw_label in label_map:
                    final_label = label_map[raw_label]
                else:
                    continue

                inputs = processor(waveform.squeeze().numpy(), sampling_rate=16000, return_tensors="pt", padding=True)
                inputs = {k: v.to(DEVICE) for k, v in inputs.items()}
                embeddings = model(**inputs).last_hidden_state.mean(dim=1).cpu()
                
                features_list.append(embeddings)
                labels_list.append(final_label)
                
                spk_str = extract_speaker_id(wav_path)
                speaker_indices_list.append(speaker_to_idx.get(spk_str, 0))
                
            except Exception as e:
                print(f"âš ï¸ Error: {wav_path} -> {e}")
                continue

    if len(features_list) == 0:
        raise ValueError("âŒ éŒ¯èª¤ï¼šæ²’æœ‰ä»»ä½•è³‡æ–™è¢«æˆåŠŸè®€å–ï¼")

    X = torch.cat(features_list, dim=0)
    y = torch.tensor(labels_list, dtype=torch.long)
    s = torch.tensor(speaker_indices_list, dtype=torch.long)
    return X, y, s, speaker_to_idx

# ==========================================
# ğŸš€ 4. ä¸»ç¨‹å¼åŸ·è¡Œ
# ==========================================
if __name__ == "__main__":
    # --- A. æº–å‚™ç‰¹å¾µ (åªè·‘ä¸€æ¬¡) ---
    print("ğŸ§  è¼‰å…¥ Wav2Vec2 æ¨¡å‹...")
    processor = Wav2Vec2Processor.from_pretrained(MODEL_NAME)
    w2v_model = Wav2Vec2Model.from_pretrained(MODEL_NAME).to(DEVICE)
    
    print("\nğŸ“¦ æº–å‚™è³‡æ–™...")
    X_train, y_train, s_train, speaker_map = prepare_data(TRAIN_CSV_PATH, processor, w2v_model, is_train=True)
    X_test, y_test, s_test, _ = prepare_data(TEST_CSV_PATH, processor, w2v_model, speaker_to_idx=speaker_map, is_train=False)
    
    num_speakers = len(speaker_map)
    train_dataset = TensorDataset(X_train, y_train, s_train)
    test_dataset = TensorDataset(X_test, y_test, s_test)
    
    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)
    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)

    # --- B. è¨“ç·´ DANN ---
    print(f"\nğŸ—ï¸ åˆå§‹åŒ– DANN æ¨¡å‹ (Class=2, Speakers={num_speakers})...")
    dann_model = DANN_Model(num_speakers=num_speakers).to(DEVICE)
    optimizer = optim.Adam(dann_model.parameters(), lr=0.001)
    
    criterion_class = nn.CrossEntropyLoss()
    criterion_domain = nn.CrossEntropyLoss()
    
    best_f1 = 0.0
    
    print("\nâš”ï¸ é–‹å§‹è¨“ç·´...")
    for epoch in range(EPOCHS):
        dann_model.train()
        total_loss = 0
        
        p = float(epoch) / EPOCHS
        alpha = 2.0 / (1.0 + np.exp(-10 * p)) - 1
        
        for inputs, labels, speakers in train_loader:
            inputs, labels, speakers = inputs.to(DEVICE), labels.to(DEVICE), speakers.to(DEVICE)
            
            optimizer.zero_grad()
            class_out, domain_out = dann_model(inputs, alpha=alpha)
            loss = criterion_class(class_out, labels) + criterion_domain(domain_out, speakers)
            loss.backward()
            optimizer.step()
            total_loss += loss.item()
        
        # è©•ä¼°
        dann_model.eval()
        correct_spk = 0
        total = 0
        all_preds = []
        all_labels = []
        
        with torch.no_grad():
            for inputs, labels, speakers in test_loader:
                inputs, labels, speakers = inputs.to(DEVICE), labels.to(DEVICE), speakers.to(DEVICE)
                c_out, d_out = dann_model(inputs, alpha=0)
                
                _, preds = torch.max(c_out, 1)
                all_preds.extend(preds.cpu().numpy())
                all_labels.extend(labels.cpu().numpy())
                
                _, spk_preds = torch.max(d_out, 1)
                correct_spk += (spk_preds == speakers).sum().item()
                total += labels.size(0)
        
        acc = accuracy_score(all_labels, all_preds)
        f1 = f1_score(all_labels, all_preds, average='macro') # ä½¿ç”¨ Macro F1
        spk_acc = correct_spk / total
        
        if f1 > best_f1:
            best_f1 = f1
            torch.save(dann_model.state_dict(), "best_dann_model.pth") # å­˜æœ€å¥½çš„æ¨¡å‹
            print(f"Epoch {epoch+1}/{EPOCHS} | Loss: {total_loss:.2f} | Dep Acc: {acc:.4f} | F1: {f1:.4f} | Spk Acc: {spk_acc:.4f} ğŸ”¥ New Best!")
        else:
            if (epoch+1) % 5 == 0:
                print(f"Epoch {epoch+1}/{EPOCHS} | Loss: {total_loss:.2f} | Dep Acc: {acc:.4f} | F1: {f1:.4f} | Spk Acc: {spk_acc:.4f}")

    # --- C. é›™é‡ t-SNE ç¹ªåœ– ---
    print("\nğŸ¨ æ­£åœ¨è¼‰å…¥æœ€ä½³æ¨¡å‹ä¸¦ç¹ªè£½ t-SNE åœ–...")
    dann_model.load_state_dict(torch.load("best_dann_model.pth")) # è¼‰å…¥æœ€ä½³æ¬Šé‡
    dann_model.eval()
    
    feats = []
    spks = []
    lbls = []
    
    with torch.no_grad():
        for inputs, labels, speakers in test_loader:
            inputs = inputs.to(DEVICE)
            f = dann_model.shared_encoder(inputs).cpu().numpy()
            feats.append(f)
            spks.extend(speakers.cpu().numpy())
            lbls.extend(labels.cpu().numpy())
            
    feats = np.vstack(feats)
    spks = np.array(spks)
    lbls = np.array(lbls)
    
    # åŸ·è¡Œä¸€æ¬¡ t-SNE
    print("â³ è¨ˆç®— t-SNE ä¸­...")
    tsne = TSNE(n_components=2, random_state=42, perplexity=30)
    feats_2d = tsne.fit_transform(feats)
    
    # åœ– 1: Speaker åˆ†ä½ˆ (è­‰æ˜å»è­˜åˆ¥åŒ–)
    plt.figure(figsize=(10, 8))
    # é€™è£¡ç‚ºäº†å±•ç¤ºæ•ˆæœï¼Œåªé¸å‰ 10 å€‹äººçš„è³‡æ–™ä¾†ç•«ï¼Œä¸ç„¶é¡è‰²å¤ªå¤šæœƒçœ‹ä¸æ¸…æ¥š
    # å¦‚æœæƒ³ç•«å…¨éƒ¨ï¼Œå°±æŠŠ mask æ‹¿æ‰
    top_speakers = pd.Series(spks).value_counts().index[:10]
    mask = np.isin(spks, top_speakers)
    
    sns.scatterplot(x=feats_2d[mask, 0], y=feats_2d[mask, 1], hue=spks[mask], palette="tab10", legend="full", s=60, alpha=0.7)
    plt.title("DANN Features by Speaker (Should be Mixed)", fontsize=16)
    plt.legend(title="Speaker ID", bbox_to_anchor=(1.05, 1), loc='upper left')
    plt.tight_layout()
    plt.savefig("tsne_speaker.png")
    print("âœ… å·²å„²å­˜: tsne_speaker.png (è«‹æª¢æŸ¥æ˜¯å¦æ··åœ¨ä¸€èµ·)")
    
    # åœ– 2: Depression åˆ†ä½ˆ (è­‰æ˜ä¿ç•™ç—…ç†ç‰¹å¾µ)
    plt.figure(figsize=(10, 8))
    # ç•«å‡ºæ‰€æœ‰é»ï¼ŒæŒ‰æ†‚é¬±ç—‡æ¨™ç±¤è‘—è‰²
    sns.scatterplot(x=feats_2d[:, 0], y=feats_2d[:, 1], hue=lbls, palette={0: 'blue', 1: 'red'}, style=lbls, s=60, alpha=0.6)
    plt.title("DANN Features by Depression (Should be Separated)", fontsize=16)
    plt.legend(title="Depression", labels=["Non-Depressed", "Depressed"])
    plt.savefig("tsne_depression.png")
    print("âœ… å·²å„²å­˜: tsne_depression.png (è«‹æª¢æŸ¥ç´…è—æ˜¯å¦åˆ†é–‹)")