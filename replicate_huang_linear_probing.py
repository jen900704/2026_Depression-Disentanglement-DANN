import os
import torch
import torch.nn as nn
import pandas as pd
import numpy as np
import torchaudio
from dataclasses import dataclass
from typing import Optional, Tuple, Dict, List, Union
from tqdm import tqdm
from sklearn.metrics import accuracy_score, f1_score, classification_report
from torch.utils.data import Dataset, DataLoader
from transformers import Wav2Vec2Processor, Wav2Vec2Config, Wav2Vec2Model, Wav2Vec2PreTrainedModel
from transformers.file_utils import ModelOutput

# ================= ğŸ”§ 1. è¨­å®šå€ (Scenario A) =================
TRAIN_CSV = "./experiment_sisman_scientific/scenario_A_screening/train.csv"
TEST_CSV = "./experiment_sisman_scientific/scenario_A_screening/test.csv"
AUDIO_ROOT = "/export/fs05/hyeh10/depression/daic_5utt_full/merged_5"

MODEL_NAME = "facebook/wav2vec2-base"
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
BATCH_SIZE = 8  # ğŸ”¥ å»ºè­°èª¿å°ä»¥é˜² OOM
EPOCHS = 30
LEARNING_RATE = 1e-3 

# ================= ğŸ§  2. Huang et al. (2024) æ¶æ§‹å®šç¾© (Frozen å¼·åŒ–ç‰ˆ) =================

@dataclass
class SpeechClassifierOutput(ModelOutput):
    loss: Optional[torch.FloatTensor] = None
    logits: torch.FloatTensor = None

class Wav2Vec2ClassificationHead(nn.Module):
    """Huang et al. å®šç¾©çš„åˆ†é¡é ­: Linear -> Tanh -> Linear"""
    def __init__(self, config):
        super().__init__()
        self.dense = nn.Linear(config.hidden_size, config.hidden_size)
        self.dropout = nn.Dropout(config.final_dropout)
        self.out_proj = nn.Linear(config.hidden_size, config.num_labels)

    def forward(self, features, **kwargs):
        x = features
        x = self.dropout(x)
        x = self.dense(x)
        x = torch.tanh(x)
        x = self.dropout(x)
        x = self.out_proj(x)
        return x

class HuangForSpeechClassification(Wav2Vec2PreTrainedModel):
    def __init__(self, config):
        super().__init__(config)
        self.num_labels = config.num_labels
        self.pooling_mode = "mean" 
        
        # 1. è¼‰å…¥ä¸»å¹¹ç¶²è·¯
        self.wav2vec2 = Wav2Vec2Model(config)
        
        # ğŸ”¥ ä¿®æ”¹é» A: åˆå§‹åŒ–æ™‚ç«‹å³å‡çµ Wav2Vec2 æ‰€æœ‰åƒæ•¸
        for param in self.wav2vec2.parameters():
            param.requires_grad = False
            
        # 2. è¼‰å…¥åˆ†é¡é ­ (é€™éƒ¨åˆ†ç¶­æŒå¯è¨“ç·´)
        self.classifier = Wav2Vec2ClassificationHead(config)
        self.init_weights()

    def forward(self, input_values, attention_mask=None, labels=None):
        # ğŸ”¥ ä¿®æ”¹é» B: ä½¿ç”¨ torch.no_grad() åŒ…è£¹ Frozen çš„ä¸»å¹¹é‹ç®—
        with torch.no_grad():
            outputs = self.wav2vec2(input_values, attention_mask=attention_mask)
            hidden_states = outputs[0]
            # Mean Pooling
            hidden_states = torch.mean(hidden_states, dim=1)
        
        # åˆ†é¡é ­ (Classifier) å¿…é ˆåœ¨ no_grad ä¹‹å¤–ï¼Œå› ç‚ºå®ƒéœ€è¦æ›´æ–°æ¬Šé‡
        logits = self.classifier(hidden_states)

        loss = None
        if labels is not None:
            loss_fct = nn.CrossEntropyLoss()
            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))

        return SpeechClassifierOutput(loss=loss, logits=logits)

# ================= ğŸ“‚ 3. è³‡æ–™è¼‰å…¥ (Scenario A) =================

class ScenarioADataset(Dataset):
    def __init__(self, csv_path, processor):
        self.df = pd.read_csv(csv_path)
        self.processor = processor
        # ğŸ”¥ ä¿®æ­£: åŠ å…¥æ¨™ç±¤å°ç…§è¡¨
        self.label_map = {
            'non': 0, '0': 0, 0: 0,
            'dep': 1, '1': 1, 1: 1
        }

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        row = self.df.iloc[idx]
        wav_path = os.path.join(AUDIO_ROOT, row['path'])
        
        # è®€å–éŸ³è¨Š
        speech, sr = torchaudio.load(wav_path)
        if sr != 16000:
            speech = torchaudio.transforms.Resample(sr, 16000)(speech)
        
        # ğŸ”¥ åŠ å…¥é•·åº¦æˆªæ–·ä»¥é˜² OOM
        MAX_LEN = 16000 * 10 # 10ç§’
        if speech.shape[1] > MAX_LEN:
             speech = speech[:, :MAX_LEN]

        input_values = self.processor(speech.squeeze().numpy(), sampling_rate=16000, return_tensors="pt").input_values[0]
        
        # --- ä¿®æ­£å¾Œçš„ Label è™•ç†é‚è¼¯ ---
        raw_label = str(row['label']).strip().lower()
        if raw_label in self.label_map:
            label_int = self.label_map[raw_label]
        else:
            label_int = 0
            
        label = torch.tensor(label_int, dtype=torch.long)
        
        return {"input_values": input_values, "labels": label}

def collate_fn(batch):
    input_values = [item['input_values'] for item in batch]
    labels = [item['labels'] for item in batch]
    
    # Padding åˆ°è©² batch æœ€é•·é•·åº¦
    input_values = torch.nn.utils.rnn.pad_sequence(input_values, batch_first=True)
    labels = torch.stack(labels)
    
    return {"input_values": input_values, "labels": labels}

# ================= ğŸš€ 4. åŸ·è¡Œå¯¦é©— =================

if __name__ == "__main__":
    print(f"ğŸš€ å•Ÿå‹•å¯¦é©—: è¤‡è£½ Huang et al. æ¶æ§‹åœ¨ Scenario A åŸ·è¡Œ Linear Probing")
    
    processor = Wav2Vec2Processor.from_pretrained(MODEL_NAME)
    config = Wav2Vec2Config.from_pretrained(MODEL_NAME, num_labels=2, final_dropout=0.1)
    
    model = HuangForSpeechClassification.from_pretrained(MODEL_NAME, config=config).to(DEVICE)

    # å†æ¬¡ç¢ºèªå‡çµç‹€æ…‹
    print("â„ï¸ Wav2Vec2 Backbone å·²å‡çµã€‚åªè¨“ç·´ Classification Headã€‚")

    train_ds = ScenarioADataset(TRAIN_CSV, processor)
    test_ds = ScenarioADataset(TEST_CSV, processor)
    
    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)
    test_loader = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)

    optimizer = torch.optim.Adam(model.classifier.parameters(), lr=LEARNING_RATE)
    
    for epoch in range(EPOCHS):
        model.train()
        train_loss = 0
        for batch in tqdm(train_loader, desc=f"Epoch {epoch+1}"):
            optimizer.zero_grad()
            inputs = batch['input_values'].to(DEVICE)
            labels = batch['labels'].to(DEVICE)
            
            outputs = model(inputs, labels=labels)
            loss = outputs.loss
            loss.backward()
            optimizer.step()
            train_loss += loss.item()

        # è©•ä¼°
        model.eval()
        all_preds = []
        all_labels = []
        with torch.no_grad():
            for batch in test_loader:
                inputs = batch['input_values'].to(DEVICE)
                labels = batch['labels'].to(DEVICE)
                logits = model(inputs).logits
                preds = torch.argmax(logits, dim=1).cpu().numpy()
                all_preds.extend(preds)
                all_labels.extend(labels.cpu().numpy())
        
        acc = accuracy_score(all_labels, all_preds)
        f1 = f1_score(all_labels, all_preds)
        print(f"âœ… Epoch {epoch+1} | Loss: {train_loss/len(train_loader):.4f} | Test Acc: {acc:.4f} | Test F1: {f1:.4f}")
        
    print("\nğŸ å¯¦é©—çµæŸã€‚")
    print(classification_report(all_labels, all_preds))