import os
import pandas as pd
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
from torch.autograd import Function
from transformers import Wav2Vec2Processor, Wav2Vec2Model
import torchaudio
import torchaudio.transforms as T
from tqdm import tqdm
from sklearn.metrics import accuracy_score, f1_score

# ================= è¨­å®šå€ =================
TRAIN_CSV_PATH = "./experiment_sisman_scientific/scenario_B_monitoring/train.csv"
TEST_CSV_PATH = "./experiment_sisman_scientific/scenario_B_monitoring/test.csv"
AUDIO_ROOT = "" 

MODEL_NAME = "facebook/wav2vec2-base"
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
BATCH_SIZE = 32
EPOCHS = 20  # Baseline è·‘ 20 Epoch æ‡‰è©²å°±å¤ äº†

print(f"ğŸ–¥ï¸ ä½¿ç”¨è£ç½®: {DEVICE}")

# ================= æ¨¡å‹å®šç¾© (åŒ DANNï¼Œä½†æˆ‘å€‘ç­‰ä¸‹æœƒé—œæ‰ GRL) =================
class GradientReversalFn(Function):
    @staticmethod
    def forward(ctx, x, alpha):
        ctx.alpha = alpha
        return x.view_as(x)
    @staticmethod
    def backward(ctx, grad_output):
        output = grad_output.neg() * ctx.alpha
        return output, None

class GradientReversalLayer(nn.Module):
    def __init__(self):
        super(GradientReversalLayer, self).__init__()
    def forward(self, x, alpha=1.0):
        return GradientReversalFn.apply(x, alpha)

class DANN_Model(nn.Module):
    def __init__(self, input_dim=768, hidden_dim=128, num_classes=2, num_speakers=38):
        super(DANN_Model, self).__init__()
        self.shared_encoder = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.BatchNorm1d(hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.3)
        )
        self.class_classifier = nn.Sequential(
            nn.Linear(hidden_dim, 64),
            nn.ReLU(),
            nn.Linear(64, num_classes)
        )
        self.domain_classifier = nn.Sequential(
            nn.Linear(hidden_dim, 64),
            nn.ReLU(),
            nn.Linear(64, num_speakers)
        )
        self.grl = GradientReversalLayer()

    def forward(self, x, alpha=0.0): # é è¨­ alpha=0 (ç„¡å°æŠ—)
        features = self.shared_encoder(x)
        class_output = self.class_classifier(features)
        reverse_features = self.grl(features, alpha)
        domain_output = self.domain_classifier(reverse_features)
        return class_output, domain_output

# ================= è³‡æ–™è™•ç† (å« Augmentation) =================
def extract_speaker_id(filepath):
    filename = os.path.basename(filepath)
    speaker_id = filename.split('_')[0]
    return speaker_id

def prepare_data(csv_path, processor, model, speaker_to_idx=None, is_train=True):
    df = pd.read_csv(csv_path)
    print(f"ğŸ“‚ æ­£åœ¨è™•ç† {csv_path} (å…± {len(df)} ç­†)...")
    if is_train: print("ğŸ”¥ æ³¨æ„ï¼šæ­£åœ¨å°è¨“ç·´è³‡æ–™æ‡‰ç”¨ Random Pitch Shift (éš¨æ©Ÿè®Šèª¿)...")
    
    features_list = []
    labels_list = []
    speaker_indices_list = []
    
    label_map = {'dep': 1, '1': 1, 1: 1, 'non': 0, '0': 0, 0: 0}

    if is_train and speaker_to_idx is None:
        all_speakers = df['path'].apply(extract_speaker_id).unique()
        all_speakers = sorted(all_speakers)
        speaker_to_idx = {spk: idx for idx, spk in enumerate(all_speakers)}
    
    model.eval()
    with torch.no_grad():
        for _, row in tqdm(df.iterrows(), total=len(df), desc="Extracting"):
            wav_path = os.path.join(AUDIO_ROOT, row['path'])
            try:
                waveform, sample_rate = torchaudio.load(wav_path)
                if sample_rate != 16000:
                    waveform = torchaudio.transforms.Resample(sample_rate, 16000)(waveform)
                if waveform.shape[0] > 1: waveform = torch.mean(waveform, dim=0, keepdim=True)
                
                # ğŸ”¥ğŸ”¥ğŸ”¥ã€é—œéµä¿®æ”¹ï¼šData Augmentationã€‘ğŸ”¥ğŸ”¥ğŸ”¥
                # åªæœ‰è¨“ç·´é›†è¦åšè®Šèª¿ï¼Œæ¸¬è©¦é›†è¦ä¿æŒåŸæ¨£ï¼ˆé€™æ¨£æ‰å…¬å¹³ï¼‰
                if is_train:
                    # éš¨æ©Ÿæ±ºå®šè®Šèª¿å¤šå°‘ (-3 åˆ° +3 åŠéŸ³)
                    n_steps = torch.randint(low=-3, high=4, size=(1,)).item()
                    if n_steps != 0:
                        effects = [['pitch', str(n_steps * 100)], ['rate', '16000']]
                        waveform, _ = torchaudio.sox_effects.apply_effects_tensor(waveform, 16000, effects)

                inputs = processor(waveform.squeeze().numpy(), sampling_rate=16000, return_tensors="pt", padding=True)
                inputs = {k: v.to(DEVICE) for k, v in inputs.items()}
                embeddings = model(**inputs).last_hidden_state.mean(dim=1).cpu()
                
                # Label è™•ç†
                raw_label = str(row['label']).strip().lower()
                if raw_label in label_map:
                    final_label = label_map[raw_label]
                else:
                    continue
                
                features_list.append(embeddings)
                labels_list.append(final_label)
                spk_str = extract_speaker_id(wav_path)
                speaker_indices_list.append(speaker_to_idx.get(spk_str, 0))
                
            except Exception as e:
                # print(f"Error: {e}") # å¿½ç•¥é›œè¨Š
                continue

    X = torch.cat(features_list, dim=0)
    y = torch.tensor(labels_list, dtype=torch.long)
    s = torch.tensor(speaker_indices_list, dtype=torch.long)
    return X, y, s, speaker_to_idx

# ================= ä¸»ç¨‹å¼ =================
if __name__ == "__main__":
    print("ğŸ§  è¼‰å…¥ Wav2Vec2...")
    processor = Wav2Vec2Processor.from_pretrained(MODEL_NAME)
    w2v_model = Wav2Vec2Model.from_pretrained(MODEL_NAME).to(DEVICE)
    
    # 1. æº–å‚™è³‡æ–™
    # Train: æœƒåš Random Pitch Shift
    X_train, y_train, s_train, speaker_map = prepare_data(TRAIN_CSV_PATH, processor, w2v_model, is_train=True)
    # Test: ä¸åš Augmentation (å…¬å¹³æ¯”è¼ƒ)
    X_test, y_test, s_test, _ = prepare_data(TEST_CSV_PATH, processor, w2v_model, speaker_to_idx=speaker_map, is_train=False)
    
    train_loader = DataLoader(TensorDataset(X_train, y_train, s_train), batch_size=BATCH_SIZE, shuffle=True)
    test_loader = DataLoader(TensorDataset(X_test, y_test, s_test), batch_size=BATCH_SIZE, shuffle=False)
    
    # 2. åˆå§‹åŒ–æ¨¡å‹
    num_speakers = len(speaker_map)
    print(f"\nğŸ—ï¸ åˆå§‹åŒ– Augmentation Baseline æ¨¡å‹ (Class=2, Speakers={num_speakers})...")
    model = DANN_Model(num_speakers=num_speakers).to(DEVICE)
    
    optimizer = optim.Adam(model.parameters(), lr=0.001)
    criterion_class = nn.CrossEntropyLoss()
    criterion_domain = nn.CrossEntropyLoss()
    
    print("\nâš”ï¸ é–‹å§‹ Augmentation Baseline è¨“ç·´...")
    print("âš ï¸ æ³¨æ„ï¼šé€™è£¡ alpha=0ï¼Œæ‰€ä»¥èº«åˆ†åˆ†é¡å™¨ (Speaker Head) åªè² è²¬ç›£æ¸¬ï¼Œä¸å½±éŸ¿ Encoderï¼")
    
    for epoch in range(EPOCHS):
        model.train()
        total_loss = 0
        
        for inputs, labels, speakers in train_loader:
            inputs, labels, speakers = inputs.to(DEVICE), labels.to(DEVICE), speakers.to(DEVICE)
            
            optimizer.zero_grad()
            
            # ğŸ”¥ é—œéµï¼šalpha=0ï¼Œåˆ‡æ–· GRL æ¢¯åº¦
            class_out, domain_out = model(inputs, alpha=0.0)
            
            # Loss åªç®— Class Loss (å› ç‚ºæˆ‘å€‘ä¸æ˜¯åœ¨åš DANNï¼Œåªæ˜¯åœ¨åšæ™®é€šè¨“ç·´)
            # ä½†æˆ‘å€‘é‚„æ˜¯ç®— domain_loss ä¾†è®“ Speaker Head å­¸ç¿’ (ä½œç‚º Probe)
            loss_class = criterion_class(class_out, labels)
            loss_domain = criterion_domain(domain_out, speakers) 
            
            # Backprop: é€™è£¡æˆ‘å€‘ã€Œåªå° Class Lossã€åšå„ªåŒ–ï¼Œè®“ Speaker Head è‡ªå·±ç©
            # é€™æ¨£ Speaker Head å°±æœƒè®Šæˆä¸€å€‹ã€Œèª å¯¦çš„è©•åˆ†å“¡ã€ï¼Œå‘Šè¨´æˆ‘å€‘é‚„å‰©å¤šå°‘èº«åˆ†è³‡è¨Š
            loss = loss_class + loss_domain 
            
            loss.backward()
            optimizer.step()
            total_loss += loss.item()
            
        # è©•ä¼°
        model.eval()
        correct_spk = 0
        total = 0
        all_preds = []
        all_labels = []
        
        with torch.no_grad():
            for inputs, labels, speakers in test_loader:
                inputs, labels, speakers = inputs.to(DEVICE), labels.to(DEVICE), speakers.to(DEVICE)
                c_out, d_out = model(inputs, alpha=0.0)
                
                _, preds = torch.max(c_out, 1)
                all_preds.extend(preds.cpu().numpy())
                all_labels.extend(labels.cpu().numpy())
                
                _, spk_preds = torch.max(d_out, 1)
                correct_spk += (spk_preds == speakers).sum().item()
                total += labels.size(0)
                
        dep_acc = accuracy_score(all_labels, all_preds)
        spk_acc = correct_spk / total
        
        print(f"Epoch {epoch+1}/{EPOCHS} | Loss: {total_loss:.2f}")
        print(f"   ğŸ‘‰ [Test] Dep Acc: {dep_acc:.4f} (é æœŸæœƒæ‰)")
        print(f"   ğŸ‘‰ [Test] Spk Acc: {spk_acc:.4f} (é æœŸé‚„å¾ˆé«˜ï¼Œä¾‹å¦‚ 0.4~0.6)")
        print("-" * 50)